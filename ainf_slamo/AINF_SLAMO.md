# AINF_SLAMO â€” Technical Documentation

**Active Inference SLAM with LiDAR, MPPI Navigation and a Known Room Model**
Version 2.0 â€” March 2026

---

## Table of Contents

1. [Overview](#1-overview)
2. [System Architecture](#2-system-architecture)
3. [State Space](#3-state-space)
4. [Active Inference Framework](#4-active-inference-framework)
5. [Generative Model â€” Room SDF](#5-generative-model--room-sdf)
6. [Variational Free Energy](#6-variational-free-energy)
7. [Dual-Prior Fusion (Command + Odometry)](#7-dual-prior-fusion-command--odometry)
8. [EKF-Style Covariance Propagation](#8-ekf-style-covariance-propagation)
9. [Variational Update â€” Gradient Descent on F](#9-variational-update--gradient-descent-on-f)
10. [Covariance Update (Bayesian Fusion)](#10-covariance-update-bayesian-fusion)
11. [Innovation and Health Monitoring](#11-innovation-and-health-monitoring)
12. [Velocity-Adaptive Gradient Weights](#12-velocity-adaptive-gradient-weights)
13. [Pose Smoothing (EMA Filter)](#13-pose-smoothing-ema-filter)
14. [Prediction-Based Early Exit](#14-prediction-based-early-exit)
15. [Initialisation and Kidnapping Recovery](#15-initialisation-and-kidnapping-recovery)
16. [Polygon Path Planner](#16-polygon-path-planner)
17. [MPPI Trajectory Controller](#17-mppi-trajectory-controller)
18. [Complete Algorithm Loop](#18-complete-algorithm-loop)
19. [Parameters Reference](#19-parameters-reference)
20. [Key Implementation Notes](#20-key-implementation-notes)

---

## 1. Overview

**AINF_SLAMO** estimates the 2-D pose of a robot inside a room whose geometry is
known *a priori* (the *nominal model*) and navigates through it.  Sensor data
comes from a 3-D LiDAR whose returns are projected onto the ground plane.

The system has two main subsystems:

- **Localisation** â€” grounded in **Active Inference** (Karl Friston, 2010): the
  system minimises a *Variational Free Energy* functional that combines a sensor
  **likelihood** (how well the LiDAR hits the room walls) with a motion-model
  **prior** (where the odometry says the robot should be).  Differentiation is
  done automatically by **LibTorch** (C++ PyTorch).

- **Navigation** â€” an **MPPI (Model Predictive Path Integral)** trajectory
  controller generates smooth velocity commands using the LiDAR-based ESDF
  (Euclidean Signed Distance Field).  The controller uses **warm-start**,
  **AR(1) correlated noise**, and **structured exploration injections** to handle
  both open corridors and narrow doorways.

---

## 2. System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Webots2Robocomp   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Webots sim   â”‚ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â–º â”‚  read_lidar() thread â”‚
â”‚  (GT only,    â”‚  (optional, for     â”‚  â€¢ GT pose (optional) â”‚
â”‚   debug)      â”‚   use_webots=true)  â”‚  â€¢ LiDAR 3D points   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚ buffer_sync
                                                 â–¼
  JoystickAdapter â”€â”€â”€ cmd velocity â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚   compute() @ 20 Hz  â”‚
  FullPoseEstimationPub               â”‚   RoomConceptAI::     â”‚
    (encoders/IMU) â”€â”€ odom velocity â”€â–ºâ”‚     update()          â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â–¼                           â–¼         â–¼                  â–¼
          Dual-Prior Fusion           EKF Predict   VFE minim.    Covariance update
          (cmd âŠ• odometry)            (propagate)   (Adam optim)  (Bayesian fusion)

  Shift+Right click â”€â”€â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    (set target)           â”‚  PolygonPathPlanner   â”‚â”€â”€â”€â–º path waypoints
                           â”‚  (Visibility Graph)   â”‚         â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â–¼
                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          LiDAR points â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  TrajectoryController â”‚
                          Robot pose â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  (MPPI + ESDF)        â”‚
                                                 â”‚  â€¢ Warm-start seq.    â”‚
                                                 â”‚  â€¢ AR(1) sampling     â”‚
                                                 â”‚  â€¢ Injection seeds    â”‚
                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                            â”‚
                                                            â–¼
                                                   (adv, rot) â†’ robot
```

> **Note:** The Webots GT pose is used **only** for debug error statistics
> (GT Î”xy, GT Î”Î¸ displays). The localisation algorithm relies exclusively on
> LiDAR + dual odometry priors. Set `use_webots = false` in `etc/config` for
> real robot operation.

---

## 3. State Space

The optimised state is the **robot pose in the room frame**:

$$\mathbf{s} = \begin{bmatrix} x \\ y \\ \phi \end{bmatrix} \in \mathbb{R}^2 \times [-\pi,\pi)$$

where $(x, y)$ is the robot position and $\phi$ is its heading, both expressed
relative to the room origin (centre of the nominal polygon).

The room geometry (polygon vertices) is **fixed** â€” only the robot pose is
optimised.

The full 5-D internal state vector, used for backward compatibility, is:

$$\mathbf{q} = [\underbrace{w,\, l}_{\text{room dims (fixed)}},\; \underbrace{x,\, y,\, \phi}_{\text{robot pose (optimised)}}]$$

---

## 4. Active Inference Framework

Active Inference casts perception as **variational Bayesian inference**.
The brain (or robot) maintains a *generative model* $p(\mathbf{o} \mid \mathbf{s})$
and a recognition density $q(\mathbf{s})$.  It minimises the
**Variational Free Energy**:

$$\mathcal{F}[q] = \underbrace{-\mathbb{E}_{q}[\ln p(\mathbf{o} \mid \mathbf{s})]}_{\text{Likelihood (sensory fit)}}
                 + \underbrace{D_{\mathrm{KL}}[q(\mathbf{s}) \,\|\, p(\mathbf{s})]}_{\text{Complexity (prior fit)}}$$

In the Laplace (Gaussian) approximation $q(\mathbf{s}) = \mathcal{N}(\pmb{\mu}, \pmb{\Sigma})$,
this reduces to:

$$\mathcal{F} = \frac{1}{2\sigma_{\mathrm{obs}}^2}\,\mathcal{L}_{\mathrm{lik}}(\pmb{\mu})
              + \frac{1}{2}(\pmb{\mu} - \pmb{\mu}_{\mathrm{prior}})^{\top}
                \pmb{\Pi}_{\mathrm{prior}}
                (\pmb{\mu} - \pmb{\mu}_{\mathrm{prior}})$$

where:

| Symbol | Meaning |
|--------|---------|
| $\pmb{\mu} = [x, y, \phi]^{\top}$ | Current mean state (robot pose) |
| $\pmb{\mu}_{\mathrm{prior}}$ | Predicted pose from odometry |
| $\pmb{\Pi}_{\mathrm{prior}} = \pmb{\Sigma}_{\mathrm{prior}}^{-1}$ | Precision (inverse predicted covariance) |
| $\sigma_{\mathrm{obs}} = 0.05\,\mathrm{m}$ | Observation noise std (5 cm) |
| $\mathcal{L}_{\mathrm{lik}}$ | Huber-robust LiDAR likelihood loss |

---

## 5. Generative Model â€” Room SDF

The generative model relates robot pose to expected LiDAR observations via a
**Signed Distance Function (SDF)** of the room boundary.

### 5.1 Coordinate Transform

LiDAR returns arrive in the **robot frame** $\mathbf{p}^r = (p^r_x, p^r_y)$.
They are mapped to the **room frame** by:

$$\mathbf{p}^{\mathrm{room}} = R(\phi)\,\mathbf{p}^r + \mathbf{t}$$

where $R(\phi) \in SO(2)$ is the 2-D rotation matrix for heading $\phi$ and
$\mathbf{t} = (x, y)^{\top}$ is the robot translation.

Explicitly:

$$\begin{bmatrix} p^{\mathrm{room}}_x \\ p^{\mathrm{room}}_y \end{bmatrix}
= \begin{bmatrix} \cos\phi & -\sin\phi \\ \sin\phi & \cos\phi \end{bmatrix}
  \begin{bmatrix} p^r_x \\ p^r_y \end{bmatrix}
+ \begin{bmatrix} x \\ y \end{bmatrix}$$

### 5.2 Polygon SDF

For a room defined by a polygon with $N$ vertices
$\mathbf{v}_0, \mathbf{v}_1, \ldots, \mathbf{v}_{N-1}$, the SDF of a point
$\mathbf{p}$ is the distance to the nearest edge:

$$\mathrm{SDF}(\mathbf{p}) = \min_{i=0}^{N-1} d\!\left(\mathbf{p},\, \overline{\mathbf{v}_i \mathbf{v}_{i+1 \bmod N}}\right)$$

The distance from $\mathbf{p}$ to segment $[\mathbf{a}, \mathbf{b}]$ is:

$$d(\mathbf{p}, [\mathbf{a},\mathbf{b}]) = \left\|\mathbf{p} - \bigl(\mathbf{a} + t^*(\mathbf{b}-\mathbf{a})\bigr)\right\|_2$$

where the clamped parameter is:

$$t^* = \mathrm{clamp}\!\left(\frac{(\mathbf{p}-\mathbf{a})\cdot(\mathbf{b}-\mathbf{a})}{\|\mathbf{b}-\mathbf{a}\|^2},\; 0,\; 1\right)$$

Implemented with a small $\epsilon$ for numerical stability:

$$d = \sqrt{\min_i d_i^2 + \varepsilon}, \quad \varepsilon = 10^{-8}$$

### 5.3 Box SDF (rectangular rooms)

For an axis-aligned box with half-extents $(h_w, h_l, h_z)$, the signed distance is:

$$\mathrm{SDF}(\mathbf{p}) = \left\|\max(\mathbf{d}, \mathbf{0})\right\|_2 + \min\!\left(\max(d_x, d_y, d_z),\; 0\right)$$

where $\mathbf{d} = |\mathbf{p}| - (h_w, h_l, h_z)^{\top}$.

---

## 6. Variational Free Energy

The total loss minimised at each step is:

$$\mathcal{F} = \mathcal{L}_{\mathrm{lik}} + \mathcal{L}_{\mathrm{prior}}$$

### 6.1 Likelihood Term

A **Huber-robust** negative log-likelihood over $N$ LiDAR points:

$$\mathcal{L}_{\mathrm{lik}} = \frac{1}{2\sigma_{\mathrm{obs}}^2} \cdot
\frac{1}{N}\sum_{i=1}^{N} \ell_\delta\!\left(\mathrm{SDF}(\mathbf{p}_i^{\mathrm{room}}),\, 0\right)$$

where the Huber loss with threshold $\delta = 0.15\,\mathrm{m}$ is:

$$\ell_\delta(r) = \begin{cases}
\frac{1}{2}r^2 & |r| \le \delta \\
\delta\!\left(|r| - \frac{\delta}{2}\right) & |r| > \delta
\end{cases}$$

Parameters: $\sigma_{\mathrm{obs}} = 0.05\,\mathrm{m}$, $\delta = 0.15\,\mathrm{m}$.

The Huber loss makes the system robust to spurious LiDAR returns (furniture,
people) which would otherwise pull the robot pose towards incorrect positions.

### 6.2 Prior Term (Active Inference complexity cost)

The Mahalanobis distance between the current pose and the predicted pose:

$$\mathcal{L}_{\mathrm{prior}} = \frac{1}{2}\,(\mathbf{s} - \pmb{\mu}_{\mathrm{pred}})^{\top}
\pmb{\Pi}_{\mathrm{prior}}\,
(\mathbf{s} - \pmb{\mu}_{\mathrm{pred}})$$

where $\pmb{\Pi}_{\mathrm{prior}} = \pmb{\Sigma}_{\mathrm{pred}}^{-1}$ is the
precision matrix of the propagated covariance.

This term anchors the solution to the motion model: if the LiDAR is ambiguous
(e.g. symmetric room), the prior prevents the pose from drifting arbitrarily.

---

## 7. Dual-Prior Fusion (Command + Odometry)

Between two consecutive LiDAR scans at times $t_{k-1}$ and $t_k$, **two
independent velocity sources** are integrated to produce two motion priors:

| Source | Interface | Variables | Nature |
|--------|-----------|-----------|--------|
| **Commanded velocity** | `JoystickAdapter` | `adv_x`, `adv_y`, `rot` | Open-loop intent (before robot moves) |
| **Measured odometry** | `FullPoseEstimationPub` | `adv`, `side`, `rot` | Closed-loop feedback from encoders/IMU |

Both are in the **robot frame** (m/s, rad/s).

### 7.1 Velocity Integration

Each source is integrated over the time window $[t_{k-1}, t_k]$ using the
velocity command/reading buffer, segment-by-segment, with a running heading
$\theta(t)$:

$$\Delta\mathbf{s} = \sum_{j} \int_{t_{j}}^{t_{j+1}} \begin{bmatrix}
    v^r_x \cos\theta(t) - v^r_y \sin\theta(t) \\
    v^r_x \sin\theta(t) + v^r_y \cos\theta(t) \\
    -\omega
\end{bmatrix} dt$$

This yields two predicted poses:

$$\pmb{\mu}_{\mathrm{cmd}} = \pmb{\mu}_{k-1} + \Delta\mathbf{s}_{\mathrm{cmd}}, \qquad
\pmb{\mu}_{\mathrm{odom}} = \pmb{\mu}_{k-1} + \Delta\mathbf{s}_{\mathrm{odom}}$$

### 7.2 Per-Prior Covariance

Each prior has its own noise model with separate configurable parameters:

$$\sigma_{\mathrm{pos}} = \sigma_{\mathrm{base}} + k_{\mathrm{trans}} \cdot |\Delta\mathbf{t}|, \qquad
\sigma_{\theta} = \sigma_{\theta,\mathrm{base}} + k_{\mathrm{rot}} \cdot |\Delta\phi|$$

| Parameter | Command prior | Odometry prior |
|-----------|:---:|:---:|
| $k_{\mathrm{trans}}$ | `cmd_noise_trans` = 0.20 | `odom_noise_trans` = 0.08 |
| $k_{\mathrm{rot}}$ | `cmd_noise_rot` = 0.10 | `odom_noise_rot` = 0.04 |
| $\sigma_{\mathrm{base}}$ | `cmd_noise_base` = 0.05 m | `odom_noise_base` = 0.01 m |

The odometry prior is approximately **2.5Ã— more precise** than the command
prior, reflecting that measured velocities are more trustworthy than
commanded velocities (which are subject to slip, motor lag, etc.).

### 7.3 Bayesian Gaussian Fusion

The two priors are fused into a single Gaussian before being passed to the
optimiser as the Active Inference prior:

$$\pmb{\Pi}_{\mathrm{fused}} = \pmb{\Sigma}_{\mathrm{cmd}}^{-1} + \pmb{\Sigma}_{\mathrm{odom}}^{-1}$$

$$\pmb{\mu}_{\mathrm{fused}} = \pmb{\Sigma}_{\mathrm{fused}} \left(
    \pmb{\Sigma}_{\mathrm{cmd}}^{-1}\,\pmb{\mu}_{\mathrm{cmd}} +
    \pmb{\Sigma}_{\mathrm{odom}}^{-1}\,\pmb{\mu}_{\mathrm{odom}}
\right)$$

$$\pmb{\Sigma}_{\mathrm{fused}} = \pmb{\Pi}_{\mathrm{fused}}^{-1}$$

The angle component is unwrapped relative to the command prior before fusion
to avoid discontinuities near $\pm\pi$.

When **no odometry readings** are available (e.g. disconnected sensor), the
system gracefully falls back to the command-only prior.

### 7.4 Odometry Noise Injection

For simulation and robustness testing, configurable Gaussian noise can be
added to the measured odometry readings:

$$v'_i = v_i + \mathcal{N}(0,\; |v_i| \cdot \alpha_{\mathrm{noise}})$$

where $\alpha_{\mathrm{noise}} =$ `ODOMETRY_NOISE_FACTOR` (default 0.1 = 10%
of reading magnitude). This allows evaluating system performance under
degraded odometry conditions.

---

## 8. EKF-Style Covariance Propagation

The uncertainty of the robot pose is tracked as a $3\times3$ covariance matrix
$\pmb{\Sigma}$.  During prediction, it is propagated by the linearised
motion model (**EKF predict** step):

$$\pmb{\Sigma}_{\mathrm{pred}} = \mathbf{F}\,\pmb{\Sigma}_{k-1}\,\mathbf{F}^{\top} + \mathbf{Q}$$

### 8.1 Motion Model Jacobian

For state $[x, y, \phi]$, the Jacobian $\mathbf{F}$ of the nonlinear motion model
$\mathbf{s}' = f(\mathbf{s}, \mathbf{u})$ w.r.t. $\mathbf{s}$ is:

$$\mathbf{F} = \mathbf{I}_3 + \begin{bmatrix}
0 & 0 & -\Delta y^l \sin\theta - \Delta x^l \cos\theta \\
0 & 0 &  \Delta y^l \cos\theta - \Delta x^l \sin\theta \\
0 & 0 & 0
\end{bmatrix}$$

where $(\Delta x^l, \Delta y^l)$ is the odometry displacement in the **robot frame**
(obtained by inverse-rotating the global delta).

### 8.2 Process Noise

The noise covariance $\mathbf{Q}$ is **anisotropic** and motion-dependent:

$$\pmb{\sigma}_{\mathrm{fwd}} = \sigma_{\mathrm{base}} + k_{\mathrm{trans}}\,|\Delta y^l|$$
$$\pmb{\sigma}_{\mathrm{lat}} = \sigma_{\mathrm{base}} + 0.3\,k_{\mathrm{trans}}\,|\Delta x^l|$$
$$\pmb{\sigma}_{\theta} = \sigma_{\theta,\mathrm{base}} + k_{\mathrm{rot}}\,|\Delta\phi|$$

Transformed to the global frame:

$$\mathbf{Q} = \begin{bmatrix}
\sigma_l^2 \sin^2\!\theta + \sigma_f^2 \cos^2\!\theta &
(\sigma_f^2 - \sigma_l^2)\cos\theta\sin\theta & 0 \\
(\sigma_f^2 - \sigma_l^2)\cos\theta\sin\theta &
\sigma_l^2 \cos^2\!\theta + \sigma_f^2 \sin^2\!\theta & 0 \\
0 & 0 & \sigma_\theta^2
\end{bmatrix}$$

When **stationary** ($|\Delta\mathbf{s}| < 0.01\,\mathrm{m}$), a very tight base
noise ($\sigma_{\mathrm{base}} = 0.001\,\mathrm{m}$) prevents spurious covariance
growth.

Default parameters (command prior, used for EKF propagation):

| Symbol | Value | Param name |
|--------|-------|------------|
| $\sigma_{\mathrm{base}}$ | $0.05\,\mathrm{m}$ | `cmd_noise_base` |
| $k_{\mathrm{trans}}$ | $0.20$ | `cmd_noise_trans` |
| $k_{\mathrm{rot}}$ | $0.10$ | `cmd_noise_rot` |

---

## 9. Variational Update â€” Gradient Descent on $\mathcal{F}$

Given the predicted pose as initialisation, the Adam optimiser minimises
$\mathcal{F}$ w.r.t. $(x, y, \phi)$:

$$\pmb{\mu}_{k} = \arg\min_{x,y,\phi}\;\mathcal{F}(x, y, \phi)$$

Two separate **Adam** parameter groups are used, one for position and one for
rotation, allowing different learning rates:

$$\eta_{\mathrm{pos}} = 0.05\,\mathrm{m}, \qquad \eta_{\mathrm{rot}} = 0.01\,\mathrm{rad}$$

**Early convergence** is detected when either:

- $\mathcal{F} < \tau_{\mathrm{thresh}} = 0.10$ (absolute threshold), or
- $|\mathcal{F}_{i-1} - \mathcal{F}_i| < 0.01\,\mathcal{F}_{i-1}$ (relative, after iteration 5).

Maximum iterations: $n_{\mathrm{max}} = 50$.

The gradients are scaled by **velocity-adaptive weights** $\mathbf{w}$ (see Â§12)
before the optimiser step.

---

## 10. Covariance Update (Bayesian Fusion)

After optimisation, the posterior covariance is updated using a
**Gauss-Newton Hessian approximation** of the likelihood:

$$\mathbf{H}_{\mathrm{lik}} \approx \mathrm{diag}\!\left(\frac{|\nabla_i \mathcal{L}_{\mathrm{lik}}| \cdot N_{\mathrm{pts}}}{\mathcal{L}_{\mathrm{lik}} + \varepsilon}\right), \quad \varepsilon = 10^{-6}$$

This is the **Fisher Information** approximation:
$\mathbf{H}_{ii} \approx \mathbb{E}\!\left[(\partial_i \ln p)^2\right]$.

The **posterior precision** fuses prior precision and likelihood curvature:

$$\pmb{\Pi}_{\mathrm{post}} = \pmb{\Pi}_{\mathrm{prior}} + \mathbf{H}_{\mathrm{lik}} + \lambda\,\mathbf{I}$$

with regularisation $\lambda = 10^{-4}$ for numerical stability.

The **posterior covariance** is:

$$\pmb{\Sigma}_{k} = \pmb{\Pi}_{\mathrm{post}}^{-1}$$

This mirrors the **Kalman gain** update:

$$\pmb{\Sigma}_{k} = (\pmb{\Sigma}_{\mathrm{pred}}^{-1} + \mathbf{H}_{\mathrm{lik}})^{-1}$$

A **condition number** check $\kappa(\pmb{\Pi}_{\mathrm{post}}) < 10^6$ guards
against ill-conditioned updates; if violated, the propagated covariance is kept.

---

## 11. Innovation and Health Monitoring

The **innovation** (Kalman residual) is the difference between the optimised
pose and the predicted pose:

$$\pmb{\nu} = \pmb{\mu}_{k} - \pmb{\mu}_{\mathrm{pred}}
= \begin{bmatrix} x_k - \hat{x} \\ y_k - \hat{y} \\ \phi_k - \hat{\phi} \end{bmatrix}$$

The angle difference is normalised to $[-\pi, \pi]$.

The **innovation norm** (position only):

$$\|\pmb{\nu}_{xy}\| = \sqrt{\nu_x^2 + \nu_y^2}$$

is displayed in the UI as a health indicator:

| Colour | Value | Meaning |
|--------|-------|---------|
| ðŸŸ¢ Green | $< 5\,\mathrm{cm}$ | Prediction excellent, system healthy |
| ðŸŸ¡ Yellow | $5$â€“$15\,\mathrm{cm}$ | Moderate prediction error |
| ðŸ”´ Red | $> 15\,\mathrm{cm}$ | Large discrepancy â€” possible slip or kidnapping |

A large innovation with a small SDF loss indicates that the **LiDAR correction
overrode the odometry prediction** â€” a sign of odometry slip or environmental
change.

---

## 12. Velocity-Adaptive Gradient Weights

To improve responsiveness, the gradients of $\mathcal{F}$ w.r.t. $(x, y, \phi)$
are scaled by **motion-profile weights** $\mathbf{w} = (w_x, w_y, w_\phi)$
before the Adam update:

$$\tilde{g}_i \leftarrow w_i \cdot g_i, \qquad i \in \{x, y, \phi\}$$

The weights are computed from the velocity command history:

| Motion profile | $w_x$ | $w_y$ | $w_\phi$ |
|----------------|-------|-------|---------|
| Pure rotation ($\omega > \omega_{\mathrm{th}}$, $v < v_{\mathrm{th}}$) | $0.5$ | $0.5$ | $2.0$ |
| Pure forward ($v > v_{\mathrm{th}}$, $\omega < \omega_{\mathrm{th}}$, $v_y > v_x$) | $1.0$ | $2.0$ | $0.5$ |
| Pure lateral ($v_x > v_y$) | $2.0$ | $1.0$ | $0.5$ |
| Combined motion | $1.2$ | $1.2$ | $1.2$ |
| Stationary | $1.0$ | $1.0$ | $1.0$ |

Thresholds: $v_{\mathrm{th}} = 0.05\,\mathrm{m/s}$,
$\omega_{\mathrm{th}} = 0.05\,\mathrm{rad/s}$.

Transitions are smoothed by an **Exponential Moving Average (EMA)**:

$$\mathbf{w}_{k} = (1 - \alpha_w)\,\mathbf{w}_{k-1} + \alpha_w\,\mathbf{w}_{\mathrm{new}}, \qquad \alpha_w = 0.3$$

---

## 13. Pose Smoothing (EMA Filter)

The raw optimised pose $(\hat{x}_k, \hat{y}_k, \hat{\phi}_k)$ is filtered by an
**Exponential Moving Average** to reduce high-frequency jitter:

$$x_k^s = \alpha\,x_{k-1}^s + (1-\alpha)\,\hat{x}_k$$
$$y_k^s = \alpha\,y_{k-1}^s + (1-\alpha)\,\hat{y}_k$$

For the heading, the angle wrap-around is handled explicitly:

$$\delta\phi = \hat{\phi}_k - \phi_{k-1}^s, \quad \delta\phi \leftarrow \mathrm{wrap}(\delta\phi)$$
$$\phi_k^s = \phi_{k-1}^s + (1-\alpha)\,\delta\phi$$

Smoothing factor: $\alpha = 0.3$ (i.e. 70% weight on the new measurement).

The smoothed pose replaces the raw optimised values in the output and is stored
as the initialisation for the next prediction step.

---

## 14. Prediction-Based Early Exit

When the system is well-localised, the odometry prediction alone may be
accurate enough.  If the **predicted pose** has a small mean SDF error:

$$\bar{e}_{\mathrm{pred}} = \frac{1}{N}\sum_{i=1}^{N} |\mathrm{SDF}(\mathbf{p}_i^{\mathrm{room,pred}})| < \sigma_{\mathrm{sdf}} \cdot \gamma$$

the full Adam optimisation is **skipped entirely**, saving ~60 % CPU.

Conditions for early exit:

1. $k > k_{\mathrm{min}} = 20$ (system must be stable first)
2. $\mathrm{tr}(\pmb{\Sigma}_{xy}) < \epsilon_{\mathrm{cov}} = 0.1\,\mathrm{m}^2$ (low uncertainty)
3. $\bar{e}_{\mathrm{pred}} < \sigma_{\mathrm{sdf}} \cdot \gamma$ with $\sigma_{\mathrm{sdf}} = 0.15\,\mathrm{m}$, $\gamma = 0.5$ (threshold â‰ˆ 7.5 cm)

When early exit fires, the pose is updated from the prediction alone and the
covariance from the EKF propagation (it grows slightly each frame without a
measurement update).

---

## 15. Initialisation and Kidnapping Recovery

### 15.1 Orientation Search

On the first update after initialisation, the system tests **16 candidate poses**
(4 angular offsets Ã— 4 position mirrors) to resolve the symmetry ambiguity:

$$\Phi_{\mathrm{cand}} = \{\phi_0,\;\phi_0 + 90Â°,\;\phi_0 + 180Â°,\;\phi_0 + 270Â°\}$$
$$\mathbf{t}_{\mathrm{cand}} = \{(x,y),\;(-x,y),\;(x,-y),\;(-x,-y)\}$$

The best candidate minimises $\overline{\mathrm{SDF}^2}$ over a subsample of 100
LiDAR points.

### 15.2 Grid Search (Kidnapping)

If no valid initial pose is available, a **dense grid search** is performed over
the room interior with resolution $r_g = 0.5\,\mathrm{m}$ and angle resolution
$r_\phi = 45Â°$:

$$(\hat{x}, \hat{y}, \hat{\phi}) = \arg\min_{x \in \mathcal{G}_x,\, y \in \mathcal{G}_y,\, \phi \in \mathcal{G}_\phi}
\frac{1}{N}\sum_i \mathrm{SDF}(\mathbf{p}_i)^2$$

After the search, the best pose is set and the covariance is reset to
$\pmb{\Sigma}_0 = 0.1\,\mathbf{I}_3$.

### 15.3 Last Pose Recovery

At shutdown, the last estimated pose is written to `last_pose.json`. At startup,
if this file exists, the pose is loaded and used to warm-start the optimisation,
avoiding the grid search delay.

---

## 16. Polygon Path Planner

The `PolygonPathPlanner` class (`polygon_path_planner.h/.cpp`) provides
**grid-free shortest-path planning** inside the room layout polygon.

### 16.1 Algorithm: Visibility Graph + Dijkstra

The planner computes the **exact shortest path** between two points inside a
simple (possibly concave) polygon, without any grid discretisation.

**Pipeline:**

1. **Minkowski Inward Offset** â€” The room polygon is shrunk inward by the
   robot radius (`robot_radius = 0.25 m`). This guarantees that any path within
   the shrunken polygon is collision-free for the physical robot. The algorithm
   detects polygon winding (CW/CCW), computes inward normals per edge, offsets
   the edge lines, and intersects consecutive offset lines to find the new
   vertices.

2. **Visibility Graph Construction** â€” A graph is built where nodes are the
   vertices of the shrunken polygon. An edge connects two vertices if the
   straight segment between them lies entirely inside the polygon (i.e., they
   are *mutually visible*). Visibility is checked by: (a) no intersection with
   any polygon boundary edge, and (b) the segment midpoint is inside the
   polygon (handles concavities). Adjacent polygon edges are always connected.

3. **Path Query** â€” Given a start and goal position:
   - If there is direct line of sight â†’ return `[start, goal]`.
   - Otherwise, start and goal are added as temporary nodes connected to all
     visible polygon vertices. Dijkstra's algorithm finds the shortest path.

**Complexity:** For a polygon with $n$ vertices (typically 6â€“20):
- Precomputation: $O(n^3)$ visibility checks â€” negligible for small $n$.
- Query: $O(n^2)$ to connect start/goal + $O(n \log n)$ Dijkstra.
- Memory: $O(n^2)$ for the graph â€” a few hundred entries.

### 16.2 Future: Interior Obstacles

The Visibility Graph extends naturally to **polygonal obstacles inside the room**
(furniture, columns). The only change is:

- Obstacle polygons are **expanded outward** by `robot_radius` (Minkowski outward
  offset).
- Obstacle vertices are added to the visibility graph.
- The visibility check tests against **all** boundary segments (room + obstacles).

The `plan()` interface remains unchanged.

### 16.3 User Interaction

- **Shift + Right Click** on the viewer sets a navigation target.
- The path is planned from the current robot pose to the target.
- The result is drawn as a **cyan polyline** with waypoint dots and a **red
  target marker**.

### 16.4 Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `robot_radius` | 0.25 m | Minkowski inward offset for safe clearance |
| `waypoint_reached_dist` | 0.15 m | Distance threshold to consider a waypoint reached |
| `max_path_length` | 50.0 m | Reject paths longer than this |

---

## 17. MPPI Trajectory Controller

The `TrajectoryController` class (`trajectory_controller.h/.cpp`) provides
**reactive local navigation** that follows the path produced by the
`PolygonPathPlanner` while avoiding obstacles detected by the LiDAR.

It implements a proper **Model Predictive Path Integral (MPPI)** controller
(Williams et al., 2017) with three key extensions: **warm-start sequence**,
**AR(1) temporally correlated noise**, and **structured exploration injections**.

### 17.1 MPPI Overview

At each control cycle (20 Hz), the controller:

1. Builds a local **ESDF** (Euclidean Signed Distance Field) from LiDAR points
   in the robot frame.
2. Computes a **carrot** point on the path ahead of the robot.
3. **Warm-starts**: shifts the previous optimal control sequence by one step
   and blends it with a nominal control (proportional to the carrot).
4. **Samples** $K=50$ candidate trajectories as perturbations of the
   warm-started sequence plus structured injection seeds.
5. Optionally **refines** each sample with gradient-based ESDF optimization.
6. **Scores** each trajectory using an Expected Free Energy (EFE) cost.
7. Computes the **MPPI weighted average** over the **full T-step sequence**:

$$u^*_t = \frac{\sum_{k=1}^{K} w_k \cdot u^k_t}{\sum_{k=1}^{K} w_k}, \qquad
w_k = \exp\!\left(-\frac{G_k - G_{\min}}{\lambda}\right)$$

8. Stores the full optimal sequence for the next cycle's warm-start.
9. Applies **EMA smoothing** and a **Gaussian brake** (advance modulated by
   rotation) to the first-step command before sending it to the robot.

### 17.2 Warm-Start Mechanism

The controller maintains `prev_optimal_[T]`, the full T-step optimal control
sequence from the previous cycle. Each cycle:

1. **Shift** left: `prev_optimal_[t] â† prev_optimal_[t+1]` for all $t$.
2. **Blend** with nominal control toward the carrot:

$$u^{\text{base}}_t = w \cdot u^{\text{prev}}_t + (1-w) \cdot u^{\text{nominal}}_t$$

with configurable weights ($w_{\text{adv}} = w_{\text{rot}} = 0.5$).

All K random samples are perturbations of this blended sequence, ensuring
temporal coherence â€” the plan evolves smoothly rather than being reconstructed
from scratch each cycle.

### 17.3 AR(1) Temporally Correlated Noise

Each random sample $k$ perturbs the warm-started base with an AR(1) process:

$$\varepsilon_t = \alpha\,\varepsilon_{t-1} + \sqrt{1-\alpha^2}\;\sigma\;\mathcal{N}(0,1)$$

with $\alpha = 0.75$ (default). This produces trajectories that are **smooth
curves** rather than zigzags â€” the noise changes slowly along the horizon,
generating natural arcs that a differential-drive robot can physically execute.

The noise sigmas ($\sigma_{\text{adv}}$, $\sigma_{\text{rot}}$) adapt online:

| Situation | $\sigma_{\text{adv}}$ | $\sigma_{\text{rot}}$ |
|-----------|:---:|:---:|
| Many collisions (>50%) | â†“ 0.85Ã— | â†‘ 1.10Ã— |
| Free space, far from obstacles | â†‘ 1.01Ã— | â†“ 0.95Ã— |
| Clamped to | [0.04, 0.25] m/s | [0.10, 0.60] rad/s |

This ensures **wider lateral exploration near obstacles** (high $\sigma_{\text{rot}}$)
and **tight, nodding-free control in open space** (low $\sigma_{\text{rot}}$).

### 17.4 Structured Exploration Injections

Pure Gaussian perturbations around a forward-pointing nominal tend to all point
in roughly the same direction. When there is a wall ahead, they all collide and
the weighted average collapses.

To guarantee lateral coverage, the controller **injects deterministic seeds**
at fixed angular offsets from the carrot direction. Each injection follows a
`sqrt(phase)` blending curve from the offset angle back toward the carrot:

$$\theta^{\text{desired}}_t = \theta_{\text{carrot}} + \Delta\theta \cdot \bigl(1 - \sqrt{t/T}\bigr)$$

The number of injections scales with proximity to obstacles (ESDF at robot):

| ESDF / d_safe | Injections | Offsets |
|:---:|:---:|---|
| > 1.5 (free) | 2 | Â±30Â° |
| < 1.5 | 4 | Â±30Â°, Â±60Â° |
| < 1.0 (close) | 6 | Â±30Â°, Â±60Â°, Â±90Â° |
| < 0.6 (danger) | 8 | Â±30Â°, Â±60Â°, Â±90Â°, Â±120Â° |

The remaining $K - n_{\text{inject}}$ samples are AR(1) random perturbations.
This hybrid approach combines the **temporal coherence** of MPPI with the
**guaranteed coverage** of geometric seeds.

### 17.5 ESDF (Euclidean Signed Distance Field)

A local 2-D ESDF grid is built each cycle from the LiDAR points in robot frame:

- **Resolution**: 5 cm/cell, covering Â±4 m around the robot (160Ã—160 grid).
- **Construction**: Two-pass (forward + backward) distance transform on an
  occupancy grid. Each occupied cell gets distance 0; free cells get the
  minimum distance to any occupied cell, scaled by the grid resolution.
- **Querying**: Bilinear interpolation for smooth distance values.
  Gradient via central differences, normalised to unit length.

The ESDF provides:
- **Collision detection**: trajectory is cut when ESDF < `robot_radius`.
- **Obstacle cost**: quadratic hinge penalty when ESDF < `d_safe`.
- **Gradient refinement**: `optimize_seed()` uses the ESDF gradient to push
  trajectory points away from obstacles while pulling toward the carrot.

### 17.6 Trajectory Scoring (EFE)

Each trajectory sample is scored by an **Expected Free Energy** composed of:

$$G = G_{\text{goal}} + G_{\text{obs}} + G_{\text{smooth}} + G_{\text{vel}} + G_{\text{delta}} + G_{\text{progress}} + G_{\text{collision}}$$

| Component | Formula | Weight |
|-----------|---------|--------|
| **Goal** | Endpoint distance to carrot + heading alignment + backward penalty | `lambda_goal` = 4.0 |
| **Obstacle** | Discounted sum of quadratic + exponential ESDF penalties | `lambda_obstacle` = 10.0 |
| **Smooth** | $(u_0 - u^{\text{base}}_0)^2$ â€” continuity with warm-start | `lambda_smooth` = 0.5 |
| **Velocity** | $\sum (v^2 + 4\omega^2)$ â€” penalise unnecessary rotation 4Ã— | `lambda_velocity` = 0.01 |
| **Delta** | $\sum (\Delta v^2 + 4\Delta\omega^2)$ â€” penalise jerky commands | `lambda_delta_vel` = 0.05 |
| **Progress** | Penalise per-step motion away from carrot (discounted) | `lambda_goal` |
| **Collision** | +1000 if trajectory enters ESDF < `robot_radius` | â€” |

### 17.7 Gaussian Brake

Before sending the command to the robot, the advance velocity is modulated by
the rotation magnitude:

$$v_{\text{out}} = v_{\text{smooth}} \cdot \exp\!\left(-k \cdot \left(\frac{\omega}{\omega_{\max}}\right)^2\right), \qquad k = 2$$

This causes the robot to slow down during sharp turns, staying nearly in place
during tight manoeuvres â€” critical for doorway navigation.

### 17.8 Integration with Localisation

When the trajectory controller is active, the velocity commands it sends to the
robot are also fed into the **velocity command history** used by the Active
Inference localisation. This closes the loop: the controller's commands serve as
a high-quality prediction prior for the next localisation update.

### 17.9 Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `num_samples` | 50 | K: total trajectory samples per cycle |
| `trajectory_steps` | 30 | T: prediction horizon steps |
| `trajectory_dt` | 0.15 s | Time step per horizon step |
| `mppi_lambda` | 5.0 | MPPI temperature (lower = more selective) |
| `sigma_adv` | 0.12 m/s | Initial advance noise std |
| `sigma_rot` | 0.35 rad/s | Initial rotation noise std |
| `noise_alpha` | 0.75 | AR(1) temporal correlation |
| `warm_start_adv_weight` | 0.5 | Blend weight for previous adv sequence |
| `warm_start_rot_weight` | 0.5 | Blend weight for previous rot sequence |
| `d_safe` | 0.4 m | Safety distance for obstacle penalties |
| `robot_radius` | 0.3 m | Hard collision threshold |
| `carrot_lookahead` | 1.5 m | Lookahead distance on path |
| `goal_threshold` | 0.25 m | Distance to consider goal reached |
| `optim_iterations` | 2 | ESDF gradient refinement passes per seed |
| `velocity_smoothing` | 0.3 | EMA alpha for output smoothing |
| `num_trajectories_to_draw` | 10 | Trajectories shown in viewer |

---

## 18. Complete Algorithm Loop

```
LOOP every 50 ms:

  1. READ lidar points {p_i} in robot frame, lidar timestamp t_k
     READ current velocity commands from circular buffer
     READ measured odometry readings from circular buffer

  2. ORIENTATION SEARCH (first call only)
     â”œâ”€â”€ Test 16 (position Ã— angle) candidate poses
     â””â”€â”€ Keep the one with lowest mean SDFÂ²

  3. COMMAND PRIOR  (from commanded velocity)
     â”œâ”€â”€ Î”s_cmd = integrate_velocity(cmd_history, t_{k-1}, t_k)
     â”œâ”€â”€ Î¼_cmd = Î¼_{k-1} + Î”s_cmd
     â””â”€â”€ Î£_cmd = motion_covariance(Î”s_cmd, cmd_noise_*)

  4. MEASURED ODOMETRY PRIOR  (from encoder/IMU feedback)
     â”œâ”€â”€ Î”s_odom = integrate_odometry(odom_history, t_{k-1}, t_k)
     â”œâ”€â”€ Î¼_odom = Î¼_{k-1} + Î”s_odom
     â””â”€â”€ Î£_odom = motion_covariance(Î”s_odom, odom_noise_*)

  5. DUAL-PRIOR BAYESIAN FUSION                           [Â§7.3]
     â”œâ”€â”€ Î _fused = Î£_cmdâ»Â¹ + Î£_odomâ»Â¹
     â”œâ”€â”€ Î¼_fused = Î£_fused (Î£_cmdâ»Â¹ Î¼_cmd + Î£_odomâ»Â¹ Î¼_odom)
     â””â”€â”€ (fallback: cmd-only prior if no odometry available)

  6. EKF PREDICT
     â”œâ”€â”€ F = motion Jacobian (linearised at Î¸_{k-1})     [Â§8.1]
     â”œâ”€â”€ Q = anisotropic process noise                   [Â§8.2]
     â””â”€â”€ Î£_pred = F Î£_{k-1} F^T + Q

  7. SET PRIOR for optimisation
     â”œâ”€â”€ Î _prior = Î _fused  (or Î£_predâ»Â¹ if no fusion)
     â””â”€â”€ Model.set_prediction(Î¼_fused, Î _prior)

  8. EARLY EXIT CHECK
     â”œâ”€â”€ Evaluate mean |SDF| at Î¼_fused                  [Â§14]
     â””â”€â”€ IF small enough: return smoothed prediction â†’ SKIP to 12

  9. ADAM OPTIMISATION  (minimise F = L_lik + L_prior)
     â”œâ”€â”€ Subsample lidar to â‰¤ 500 points
     â”œâ”€â”€ For each iteration:
     â”‚   â”œâ”€â”€ Compute SDF for all points (auto-diff)
     â”‚   â”œâ”€â”€ Compute F (Huber likelihood + Mahalanobis prior)
     â”‚   â”œâ”€â”€ Backward pass â†’ gradients
     â”‚   â”œâ”€â”€ Scale gradients by velocity-adaptive weights [Â§12]
     â”‚   â”œâ”€â”€ Adam step  (Î·_pos=0.05, Î·_rot=0.01)
     â”‚   â””â”€â”€ Check convergence criteria
     â””â”€â”€ â†’ optimal pose (x*, y*, Ï†*)

 10. COVARIANCE UPDATE (Bayesian fusion)
     â”œâ”€â”€ Compute âˆ‡L_lik at optimal pose                  [Â§10]
     â”œâ”€â”€ H_lik â‰ˆ diagonal Fisher information
     â”œâ”€â”€ Î _post = Î _prior + H_lik + Î»I
     â””â”€â”€ Î£_k = Î _postâ»Â¹

 11. POSE SMOOTHING (EMA)                                [Â§13]
     â””â”€â”€ (x,y,Ï†)_smooth = EMA((x*,y*,Ï†*), prev_smooth, Î±=0.3)

 12. INNOVATION
     â”œâ”€â”€ Î½ = (x,y,Ï†)_smooth - Î¼_fused                   [Â§11]
     â””â”€â”€ Display â€–Î½_xyâ€– in UI

 13. TRAJECTORY CONTROLLER (if path active)              [Â§17]
     â”œâ”€â”€ Build local ESDF from lidar points
     â”œâ”€â”€ Warm-start: shift prev_optimal_ + blend nominal
     â”œâ”€â”€ Sample K trajectories (injections + AR(1) perturbations)
     â”œâ”€â”€ Optimize each sample with ESDF gradient
     â”œâ”€â”€ Score (EFE) and compute MPPI weighted average
     â”œâ”€â”€ Store optimal sequence for next cycle
     â”œâ”€â”€ Apply EMA smoothing + Gaussian brake
     â”œâ”€â”€ Send (adv, rot) to robot
     â””â”€â”€ Feed commands into velocity_history for localisation

 14. STORE for next cycle
     â””â”€â”€ Î¼_{k} = smooth pose,  Î£_{k} = posterior cov
```

---

## 19. Parameters Reference

### Optimisation

| Parameter | Default | Description |
|-----------|---------|-------------|
| `num_iterations` | 50 | Max Adam iterations per cycle |
| `learning_rate_pos` | 0.05 m | Adam LR for position |
| `learning_rate_rot` | 0.01 rad | Adam LR for rotation |
| `min_loss_threshold` | 0.10 | Absolute early convergence threshold |
| `max_lidar_points` | 500 | Max LiDAR points used per optimisation |

### Sensor Model

| Parameter | Default | Description |
|-----------|---------|-------------|
| `sigma_obs` | 0.05 m | Observation noise std (likelihood width) |
| `huber_delta` | 0.15 m | Huber loss transition point |

### Prediction / Dual-Prior Covariance

**Command prior** (open-loop, from joystick/controller):

| Parameter | Default | Description |
|-----------|---------|-------------|
| `cmd_noise_trans` | 0.20 | Position noise per metre of motion |
| `cmd_noise_rot` | 0.10 | Rotation noise per radian |
| `cmd_noise_base` | 0.05 m | Base process noise (stationary) |

**Measured odometry prior** (closed-loop, from encoders/IMU):

| Parameter | Default | Description |
|-----------|---------|-------------|
| `odom_noise_trans` | 0.08 | Position noise per metre of motion |
| `odom_noise_rot` | 0.04 | Rotation noise per radian |
| `odom_noise_base` | 0.01 m | Base process noise (stationary) |

**Odometry noise injection** (simulation only):

| Parameter | Default | Description |
|-----------|---------|-------------|
| `ODOMETRY_NOISE_FACTOR` | 0.10 | Gaussian noise std as fraction of reading |

### Prediction-Based Early Exit

| Parameter | Default | Description |
|-----------|---------|-------------|
| `prediction_early_exit` | true | Enable early exit |
| `sigma_sdf` | 0.15 m | SDF observation noise for threshold |
| `prediction_trust_factor` | 0.5 | Threshold = Ïƒ_sdf Ã— factor |
| `min_tracking_steps` | 20 | Stabilisation steps before early exit |
| `max_uncertainty_for_early_exit` | 0.1 mÂ² | Max covariance trace |

### Velocity-Adaptive Weights

| Parameter | Default | Description |
|-----------|---------|-------------|
| `velocity_adaptive_weights` | true | Enable adaptive weighting |
| `linear_velocity_threshold` | 0.05 m/s | Minimum speed for "translating" |
| `angular_velocity_threshold` | 0.05 rad/s | Minimum speed for "rotating" |
| `weight_boost_factor` | 2.0 | Multiplier for active DOF |
| `weight_reduction_factor` | 0.5 | Multiplier for inactive DOF |
| `weight_smoothing_alpha` | 0.3 | EMA alpha for weight transitions |

### Pose Smoothing

| Parameter | Default | Description |
|-----------|---------|-------------|
| `pose_smoothing` | 0.3 | EMA alpha (0 = no smoothing, 1 = no update) |

### Path Planner

| Parameter | Default | Description |
|-----------|---------|-------------|
| `robot_radius` | 0.25 m | Minkowski inward offset (safe clearance) |
| `waypoint_reached_dist` | 0.15 m | Distance to consider waypoint reached |
| `max_path_length` | 50.0 m | Reject paths longer than this |

### MPPI Trajectory Controller

| Parameter | Default | Description |
|-----------|---------|-------------|
| `num_samples` | 50 | K: total trajectory samples per cycle |
| `trajectory_steps` | 30 | T: prediction horizon steps |
| `trajectory_dt` | 0.15 s | Time step per horizon step |
| `mppi_lambda` | 5.0 | Temperature (lower = more selective) |
| `sigma_adv` | 0.12 m/s | Initial advance noise std |
| `sigma_rot` | 0.35 rad/s | Initial rotation noise std |
| `noise_alpha` | 0.75 | AR(1) temporal correlation coefficient |
| `sigma_min_adv` / `sigma_max_adv` | 0.04 / 0.25 m/s | Adaptive sigma clamp range |
| `sigma_min_rot` / `sigma_max_rot` | 0.10 / 0.60 rad/s | Adaptive sigma clamp range |
| `warm_start_adv_weight` | 0.5 | Blend weight: prev vs nominal (advance) |
| `warm_start_rot_weight` | 0.5 | Blend weight: prev vs nominal (rotation) |
| `max_adv` | 0.6 m/s | Maximum forward velocity |
| `max_rot` | 0.8 rad/s | Maximum angular velocity |
| `d_safe` | 0.4 m | Safety distance for obstacle cost |
| `robot_radius` | 0.3 m | Hard collision threshold in ESDF |
| `carrot_lookahead` | 1.5 m | Path lookahead distance |
| `goal_threshold` | 0.25 m | Distance to consider goal reached |
| `optim_iterations` | 2 | ESDF gradient refinement passes |
| `optim_lr` | 0.05 | Learning rate for seed optimization |
| `velocity_smoothing` | 0.3 | EMA alpha for output smoothing |
| `grid_resolution` | 0.05 m | ESDF grid cell size |
| `grid_half_size` | 4.0 m | ESDF grid extent from robot |

---

## 20. Key Implementation Notes

### LibTorch Autograd
All SDF computations are implemented as differentiable PyTorch operations.
The backward pass through `sdf_polygon()` or `sdf_box()` automatically provides
$\nabla_{(x,y,\phi)}\mathcal{F}$, including gradients through the rotation
matrix construction.

### Thread Safety
LiDAR reading and pose estimation run in **separate threads**.  A
`DoubleBufferSync` class ensures the compute loop always sees a matched
(robot\_pose, lidar\_points) pair from the same timestamp, avoiding stale data.

### Coordinate Convention
- Room frame: origin at polygon centroid, Y forward, X right.
- Robot frame: origin at robot centre, Y forward (direction of advance),
  X right, Z up.
- LiDAR points arrive in **robot frame** in metres.
- The viewer Y axis is **flipped** ($y_{\mathrm{screen}} = -y_{\mathrm{room}}$) so
  that "up on screen" = "forward in room".

### PyTorch Thread Limiting
To avoid CPU overload, LibTorch is limited to 2 intra-op threads and 1
inter-op thread:
```cpp
torch::set_num_threads(2);
torch::set_num_interop_threads(1);
```

### Dual-Prior Graceful Degradation
The dual-prior fusion in Â§7 degrades gracefully:
- If **no odometry** readings arrive (e.g. `FullPoseEstimationPub` is
  disconnected), the system automatically falls back to the command-only prior,
  preserving the original single-prior behaviour.
- If **no velocity commands** arrive (robot not under joystick control), the
  command prior predicts zero motion; the measured odometry still provides a
  valid prior.
- When **both** agree the robot is stationary, the covariance stays extremely
  tight, enabling aggressive early exit and minimal CPU usage.

### Path Planner â€” Pure Geometry
The `PolygonPathPlanner` has **no dependency on PyTorch, Qt, or Boost**. It
uses only Eigen and the STL, making it lightweight and independently testable.
The visibility graph is precomputed once when the polygon changes and reused
for all path queries.

### Trajectory Controller â€” Pure C++ MPPI
The `TrajectoryController` has **no dependency on PyTorch**. It uses only Eigen,
the STL, and `<random>`. The ESDF is built and queried entirely in the robot
frame using a simple 2-D grid â€” no GPU or tensor operations. The MPPI weighted
average is computed over raw `std::vector<float>` sequences, making it
lightweight enough to run 50 samples Ã— 30 steps at 20 Hz on a single CPU core.

### Hybrid MPPI Design
The trajectory controller combines two sampling strategies:
- **Structured injection seeds** (2â€“8 deterministic lateral offsets) guarantee
  spatial coverage for obstacle avoidance and doorway navigation.
- **AR(1) Gaussian perturbations** (42â€“48 random samples around the warm-started
  sequence) provide temporal coherence and smooth convergence.

This hybrid avoids the failure modes of both pure geometric seeds (cancel-out
near obstacles) and pure random sampling (insufficient lateral coverage with
moderate K).

---

*This document was generated from the source code of `ainf_slamo` (March 2026, v2.0).*

